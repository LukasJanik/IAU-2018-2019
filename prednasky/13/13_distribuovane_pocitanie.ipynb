{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distribuovane pocitanie - Spark\n",
    "\n",
    "Spark je v sucasnosti prvy framework, po ktorom ludia idu ak chcu robit nejake distribuovane in-memory spracovanie udajov.\n",
    "\n",
    "Existuje ale velmi vela dalsich moznosti, nastrojov a technologii:\n",
    "* **Hadoop (MapReduce + HDFS)** - po tomto idu ludia ak maju fakt vela dat, ktore nevlezu do pamati ani na velkom clustri a/alebo chcu pracovat na disku\n",
    "* Storm\n",
    "* Flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mattturck.com/wp-content/uploads/2018/07/Matt_Turck_FirstMark_Big_Data_Landscape_2018_Final.png\" alt=\"BigData landscape 2018\"/>\n",
    "zdroj: http://mattturck.com/bigdata2018/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - zopar vlastnosti\n",
    "\n",
    "* In-memory spracovanie dat\n",
    "* Jednotny pristup k datam a vypoctovym prostriedkom (rovnako pisem kod ak pracujem na mojom notebooku a na celom klustri)\n",
    "* Na pozadi Scala a JVM, ale ma velmi dobre API pre Python a aj R\n",
    "* v podstate MapReduce ale v pamati a s moznosotu microbatchov na spravocanie prudov dat\n",
    "* Zaklad je RDD (Resilient distributed dataset) - kolekcia dat distribuovana na jednotlive uzly. Zaklad prace su transformacie nda datami reprezentovanymi ako RDD. Jednoducha podpora pre zakladne operacie ako map, filter, collect\n",
    "* Transformacie su lenive (lazy). Nevykonavaju sa dokedy ich nepotrebujete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://147.175.149.68:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData # toto je prklad RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.map(lambda x: x % 2 == 0)\n",
    "f.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.filter(lambda x: x % 2 == 0)\n",
    "f.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hrackarsky priklad s hladanim prvocisel\n",
    "\n",
    "Mam funkciu, ktora overuje ci je cislo prvocislo a ja ju chcem distribuovat na vela dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevzane z https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python\n",
    "\n",
    "def isprime(n):\n",
    "    n = abs(int(n))\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n",
      "Elapsed time: 2.577939033508301 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).count())\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Oneskoreny vypocet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:48\n",
      "Elapsed time: 0.019066810607910156 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nezavolal som funkciu, ktora by vracala vysledky, tak sa este nic nevykonalo. Pripravil sa len RDD s transformaciami, ktore sa vykonaju vtedy, ked to bude treba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11]\n",
      "Elapsed time: 0.07054328918457031 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).take(5))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az teraz som zavolal funkciu, ktora vyzaduje aby sa nieco aj spocitalo. Stacilo mi ale spocitat prvych par cisel, takze sa vykonala len cast vypoctu. Dalsie funkcie, ktore vracaju data su napr collect, count, ...\n",
    "\n",
    "OPatrne s tymito funkciami (hlavne s collect). Vracaju vam vsetky data, ktore su vysledkom vypoctu. Ak by ich bolo vela, tak sa ich aj tak pokusia vratit a pocitac, z ktoreho pristupujete k sparku to nemusi zvladnut (ak teda priztupujem k nejakemu vacsiemi stroju.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999983, 999979, 999961, 999959, 999953]\n",
      "Elapsed time: 2.7241902351379395 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).takeOrdered(5, key = lambda x: -x))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu som uz potreboval spocitat vsetko na to aby som to usporiadal, tak sa to muselo vykonat vsetko a chvilu to teda trvalo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skusme trochu realnejsi priklad\n",
    "\n",
    "zdroje:\n",
    "* https://github.com/jadianes/spark-py-notebooks\n",
    "* https://www.codementor.io/jadianes/python-spark-sql-dataframes-du107w74i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stiahnem si data o utokoch na pocitacovu siet\n",
    "\n",
    "Su to data charakterizujuce spojenia v sieti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"data/kddcup.data.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"data/kddcup.data_10_percent.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"data/corrected.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21M\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 1.4M Dec 11 23:39 corrected.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 2.1M Dec 11 23:32 kddcup.data_10_percent.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly  18M Dec 11 23:32 kddcup.data.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vytvorim si RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# nacitam data a nastavim im schemu\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vytvorim si DataFrame velmi podobny tomu v Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "type(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Viem robit podobne opearcie ako s Pandas, akurat distribuovane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.groupBy(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Co ked by som chcel k tym datam pristupovat cez SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cize by sme vedeli SPark pouzit napriklad na explorativnu analyzu celkom velkych objemov dat\n",
    "\n",
    "## Mame tiez kniznice, na to aby sme natrenovali aj nejake modely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zoberme si trenovacie a testovacie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 4898431\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "print(\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 311029\n"
     ]
    }
   ],
   "source": [
    "test_data_file = \"data/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print(\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nacitajme si ich ako zoznamy riadkov = pozorovania s atributmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct().collect()\n",
    "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
    "flags = csv_data.map(lambda x: x[3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pozorovanie vizera nejak takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '215',\n",
       "  '45076',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opis dat je dostupny tu\n",
    "http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taketo hodnoty nadobudaju niektore atributy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icmp', 'udp', 'tcp']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finger',\n",
       " 'netbios_dgm',\n",
       " 'name',\n",
       " 'X11',\n",
       " 'hostnames',\n",
       " 'vmnet',\n",
       " 'systat',\n",
       " 'shell',\n",
       " 'netstat',\n",
       " 'netbios_ssn',\n",
       " 'urh_i',\n",
       " 'pop_3',\n",
       " 'ldap',\n",
       " 'domain',\n",
       " 'mtp',\n",
       " 'remote_job',\n",
       " 'exec',\n",
       " 'supdup',\n",
       " 'courier',\n",
       " 'urp_i',\n",
       " 'pop_2',\n",
       " 'csnet_ns',\n",
       " 'smtp',\n",
       " 'whois',\n",
       " 'daytime',\n",
       " 'bgp',\n",
       " 'imap4',\n",
       " 'nntp',\n",
       " 'http_443',\n",
       " 'klogin',\n",
       " 'rje',\n",
       " 'IRC',\n",
       " 'link',\n",
       " 'http_8001',\n",
       " 'uucp',\n",
       " 'tftp_u',\n",
       " 'iso_tsap',\n",
       " 'uucp_path',\n",
       " 'auth',\n",
       " 'ecr_i',\n",
       " 'other',\n",
       " 'domain_u',\n",
       " 'ssh',\n",
       " 'discard',\n",
       " 'ctf',\n",
       " 'red_i',\n",
       " 'tim_i',\n",
       " 'time',\n",
       " 'login',\n",
       " 'Z39_50',\n",
       " 'ftp',\n",
       " 'telnet',\n",
       " 'ntp_u',\n",
       " 'sql_net',\n",
       " 'aol',\n",
       " 'private',\n",
       " 'gopher',\n",
       " 'efs',\n",
       " 'http_2784',\n",
       " 'ftp_data',\n",
       " 'nnsp',\n",
       " 'http',\n",
       " 'sunrpc',\n",
       " 'eco_i',\n",
       " 'harvest',\n",
       " 'kshell',\n",
       " 'echo',\n",
       " 'netbios_ns',\n",
       " 'pm_dump',\n",
       " 'printer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0', 'RSTR', 'SH', 'S1', 'S2', 'RSTOS0', 'REJ', 'OTH', 'SF', 'S3', 'RSTO']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zakodujme si kategoricke data\n",
    "\n",
    "Pre jednoduchost to teraz mozeme spravit ako keby to boli ordinalne premenne. V modeli explicitne povieme, ze su to kategoricke premenne aby to nebral ako cisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41] (label)\n",
    "    clean_line_split = line_split[0:41]\n",
    "    \n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(protocols)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[2] = services.index(clean_line_split[2])\n",
    "    except:\n",
    "        clean_line_split[2] = len(services)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "        \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = csv_data.map(create_labeled_point)\n",
    "test_data = test_csv_data.map(create_labeled_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A natrenujme rozhodovaci strom\n",
    "\n",
    "Pouzivame kniznicu **mllib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 272.709 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "t0 = time.time()\n",
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n",
    "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
    "\n",
    "print(\"Classifier trained in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predikcie sa daju spocitat takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated in 0.049 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "\n",
    "print(\"Predictions generated in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A uspesnost mozeme vyhodnotit takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 23.226 seconds. Test accuracy is 0.916\n"
     ]
    }
   ],
   "source": [
    "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "t0 = time.time()\n",
    "test_accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "\n",
    "print(\"Prediction made in {} seconds. Test accuracy is {}\".format(round(time.time() - t0,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pravidla zo stromu si vieme jednoducho vypisat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 27 nodes\n",
      "  If (feature 22 <= 33.0)\n",
      "   If (feature 25 <= 0.5)\n",
      "    If (feature 36 <= 0.48)\n",
      "     If (feature 34 <= 0.91)\n",
      "      Predict: 0.0\n",
      "     Else (feature 34 > 0.91)\n",
      "      Predict: 1.0\n",
      "    Else (feature 36 > 0.48)\n",
      "     If (feature 2 in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 1.0\n",
      "   Else (feature 25 > 0.5)\n",
      "    If (feature 3 in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 2 in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 3 not in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 38 <= 0.07)\n",
      "      Predict: 0.0\n",
      "     Else (feature 38 > 0.07)\n",
      "      Predict: 1.0\n",
      "  Else (feature 22 > 33.0)\n",
      "   If (feature 5 <= 0.0)\n",
      "    If (feature 2 in {19.0,35.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 not in {19.0,35.0})\n",
      "     If (feature 11 <= 0.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 11 > 0.0)\n",
      "      Predict: 0.0\n",
      "   Else (feature 5 > 0.0)\n",
      "    If (feature 29 <= 0.08)\n",
      "     If (feature 2 in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 29 > 0.08)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned classification tree model:\")\n",
    "print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Teraz by som vedel interpretovat cely strom. Staci mi pozriet sa co mam v jednotlivych stlpcoch.  Popis dat je dostupny tu: http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
